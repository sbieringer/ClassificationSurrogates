{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('./models/')\n",
    "\n",
    "from cond_CFM import CNF\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "from models.custom_linear_flipout import custom_LinearFlipout as LinearFlipout\n",
    "\n",
    "from jet_dataset import JetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### Define the data ###\n",
    "#######################\n",
    "\n",
    "dont_use = ['jet_sdmass', \n",
    "            'jet_tau1',\t'jet_tau2',\t'jet_tau3',\t'jet_tau4',\n",
    "            'aux_genpart_eta', 'aux_genpart_phi', 'aux_genpart_pid', 'aux_genpart_pt', 'aux_truth_match']\n",
    "\n",
    "dataset = JetDataset(\"./jet_data\",'train', del_context=dont_use)\n",
    "dataset_val = JetDataset(\"./jet_data\",'val', del_context=dont_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './data/CFM_VIB_sampling_corrected_k100_jets6/'\n",
    "\n",
    "if 'VIB' in save_dir:\n",
    "    model = CNF(1, conds = dataset.features.shape[1], n_nodes=[64] * 3, layer = LinearFlipout).to(device)\n",
    "    model.load_state_dict(torch.load(save_dir + f\"model_4000.pth\"))\n",
    "    n_stat = 11\n",
    "\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, LinearFlipout):\n",
    "            layer._dnn_to_bnn_flag = True\n",
    "            layer.auto_sample = False \n",
    "            layer.sample_weights()\n",
    "            \n",
    "    def sample_new_weights(model, n):\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, LinearFlipout):\n",
    "                layer.sample_weights()\n",
    "        return model\n",
    "    \n",
    "else:\n",
    "    path_add_AdamMCMC = '/AdamMCMC_models_lambda50/'\n",
    "    m_list_dir = save_dir + path_add_AdamMCMC\n",
    "\n",
    "    n_m_list = len([l for l in os.listdir(m_list_dir) if 'model' in l])\n",
    "    m_list = [f'AdamMCMC_model_{n}.pth' for n in range(n_m_list) if n%100==0]\n",
    "    n_stat = len(m_list)+1\n",
    "\n",
    "    model = CNF(1, conds = dataset.features.shape[1], n_nodes=[64] * 3).to(device)\n",
    "    model.load_state_dict(torch.load(save_dir + f\"model_4000.pth\"))\n",
    "    \n",
    "    def sample_new_weights(model, n):\n",
    "        model.load_state_dict(torch.load(m_list_dir + m_list[n]))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,1,21)\n",
    "alpha_quant = np.linspace(0,1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231957)\n",
    "perm = np.random.permutation(len(dataset_val.features))\n",
    "n_data = 10000\n",
    "\n",
    "dataset_val.features = dataset_val.features[perm[:n_data]]\n",
    "dataset_val.target = dataset_val.target[perm[:n_data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [52:27<00:00, 314.72s/it]\n"
     ]
    }
   ],
   "source": [
    "n_stat_alea = 1000\n",
    "batchsize = 1000\n",
    "n_points = len(dataset_val.features)\n",
    "eval = True\n",
    "\n",
    "if eval:\n",
    "    in_quantile = np.zeros((n_points, len(alpha_quant), n_stat))\n",
    "    acc = np.zeros((n_points, n_stat))\n",
    "    err_diff = np.zeros((n_points))\n",
    "    err_diff_hist = np.zeros((n_points))\n",
    "\n",
    "    for i in tqdm(range(n_points//batchsize)):\n",
    "        c = torch.Tensor(dataset_val.features[i*batchsize:(i+1)*batchsize]).to(device)\n",
    "        t = torch.sigmoid(torch.Tensor(dataset_val.target[i*batchsize:(i+1)*batchsize])*20).numpy()\n",
    "\n",
    "        z = torch.randn(n_stat_alea*batchsize, 1).to(device)\n",
    "        \n",
    "        x_gens = np.zeros((n_stat, batchsize, n_stat_alea))\n",
    "        for n in range(n_stat):\n",
    "            if n != 0:\n",
    "                sample_new_weights(model, n-1)\n",
    "\n",
    "            x_gen = model.decode(z, cond=c.repeat_interleave(n_stat_alea, dim=0))\n",
    "            x_gen = torch.sigmoid(x_gen*20).detach().cpu().numpy().reshape(batchsize, n_stat_alea)\n",
    "            x_gens[n] = x_gen\n",
    "\n",
    "            in_quantile[i*batchsize:(i+1)*batchsize, :, n] = np.quantile(x_gen, alpha_quant, axis=1).T >= t #without the sigmiod? same results\n",
    "            tresh = 0.5\n",
    "            is_top =c[:,0].detach().cpu().numpy()\n",
    "            acc[i*batchsize:(i+1)*batchsize, n] = (is_top==-1.).astype(float) + is_top*((x_gen > 0.5).sum(1)/n_stat_alea)\n",
    "\n",
    "        err_diff[i*batchsize:(i+1)*batchsize] = np.mean(x_gens.max(0)-x_gens.min(0), 1)\n",
    "\n",
    "        for j in range(len(c)):\n",
    "            hists = []\n",
    "            for n2 in range(n_stat):\n",
    "                hist, _ = np.histogram(x_gens[n2, j], bins = bins)\n",
    "                hists.append(hist)\n",
    "                \n",
    "            hists = np.array(hists)\n",
    "            hist_max = hists.max(0)\n",
    "            hist_min = hists.min(0)\n",
    "            err_diff_hist[i*batchsize+j] = np.mean(hist_max-hist_min)\n",
    "\n",
    "    empirical_coverage = np.sum(in_quantile, axis = 0)/n_points\n",
    "\n",
    "\n",
    "    with open(save_dir+'empirical_coverage.npy', 'wb') as f:\n",
    "        np.save(f, empirical_coverage)\n",
    "    with open(save_dir+'acc.npy', 'wb') as f:\n",
    "        np.save(f, acc)\n",
    "    with open(save_dir+'err_diff.npy', 'wb') as f:\n",
    "        np.save(f, err_diff)\n",
    "    with open(save_dir+'err_diff_hist.npy', 'wb') as f:\n",
    "        np.save(f, err_diff_hist)\n",
    "else:\n",
    "    with open(save_dir+'empirical_coverage.npy', 'rb') as f:\n",
    "        empirical_coverage = np.load(f)\n",
    "    with open(save_dir+'acc.npy', 'rb') as f:\n",
    "        acc = np.load(f)\n",
    "    with open(save_dir+'err_diff.npy', 'rb') as f:\n",
    "        err_diff = np.load(f)\n",
    "    with open(save_dir+'err_diff_hist.npy', 'rb') as f:\n",
    "        err_diff_hist = np.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate out of distribution input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### Define the data ###\n",
    "#######################\n",
    "\n",
    "dont_use = ['jet_sdmass', \n",
    "            'jet_tau1',\t'jet_tau2',\t'jet_tau3',\t'jet_tau4',\n",
    "            'aux_genpart_eta', 'aux_genpart_phi', 'aux_genpart_pid', 'aux_genpart_pt', 'aux_truth_match']\n",
    "\n",
    "dataset = JetDataset(\"./jet_data\",'train', del_context=dont_use)\n",
    "dataset_val = JetDataset(\"./jet_data\",'val', del_context=dont_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231957)\n",
    "perm = np.random.permutation(len(dataset_val.features))\n",
    "n_data = 1000\n",
    "\n",
    "dataset_val.features = dataset_val.features[perm[:n_data]]\n",
    "dataset_val.target = dataset_val.target[perm[:n_data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [1:14:40<00:00, 448.01s/it]\n"
     ]
    }
   ],
   "source": [
    "n_stat_alea = 1000\n",
    "batchsize = 1000\n",
    "n_points = len(dataset_val.features)\n",
    "eval = True\n",
    "load = True\n",
    "skip = 2\n",
    "n_vals = 10\n",
    "\n",
    "add_dims = [1,4,5]\n",
    "add_vals = [np.linspace(0,1000,n_vals), np.linspace(0, 4000,n_vals), np.linspace(0,200,n_vals)]\n",
    "\n",
    "if load:\n",
    "    with open(save_dir+'acc_ood.npy', 'rb') as f:\n",
    "        acc = np.load(f)\n",
    "    with open(save_dir+'err_diff_ood.npy', 'rb') as f:\n",
    "        err_diff = np.load(f)\n",
    "    print('loaded')\n",
    "else:\n",
    "    acc = np.zeros((len(add_dims), n_vals, n_points, n_stat))\n",
    "    err_diff = np.zeros((len(add_dims), n_vals, n_points))\n",
    "\n",
    "for i_dim, dim in enumerate(add_dims):\n",
    "    print(i_dim)\n",
    "    if i_dim<skip:\n",
    "        continue\n",
    "    for i_add_val, add_val in enumerate(tqdm(add_vals[i_dim])):\n",
    "        for i in range(n_points//batchsize):\n",
    "            c = torch.Tensor(dataset_val.features[i*batchsize:(i+1)*batchsize]).to(device)\n",
    "            c[:,dim] = (c[:,dim]*dataset_val.std_norm[0,dim]+add_val)/dataset_val.std_norm[0,dim]\n",
    "\n",
    "            t = torch.sigmoid(torch.Tensor(dataset_val.target[i*batchsize:(i+1)*batchsize])*20).numpy()\n",
    "\n",
    "            z = torch.randn(n_stat_alea*batchsize, 1).to(device)\n",
    "            \n",
    "            x_gens = np.zeros((n_stat, batchsize, n_stat_alea))\n",
    "            for n in range(n_stat):\n",
    "                if 'VIB' in save_dir:\n",
    "                    sample_new_weights(model, n-1)\n",
    "                else:\n",
    "                    if n != 0:\n",
    "                        sample_new_weights(model, n-1)\n",
    "                    else:\n",
    "                        model.load_state_dict(torch.load(save_dir + f\"model_4000.pth\"))\n",
    "\n",
    "                x_gen = model.decode(z, cond=c.repeat_interleave(n_stat_alea, dim=0))\n",
    "                x_gen = torch.sigmoid(x_gen*20).detach().cpu().numpy().reshape(batchsize, n_stat_alea)\n",
    "                x_gens[n] = x_gen\n",
    "\n",
    "                tresh = 0.5\n",
    "                is_top =c[:,0].detach().cpu().numpy()\n",
    "                acc[i_dim, i_add_val, i*batchsize:(i+1)*batchsize, n] = (is_top==-1.).astype(float) + is_top*((x_gen > 0.5).sum(1)/n_stat_alea)\n",
    "\n",
    "            err_diff[i_dim, i_add_val, i*batchsize:(i+1)*batchsize] = np.mean(x_gens.max(0)-x_gens.min(0), 1)\n",
    "\n",
    "        with open(save_dir+'acc_ood.npy', 'wb') as f:\n",
    "            np.save(f, acc)\n",
    "        with open(save_dir+'err_diff_ood.npy', 'wb') as f:\n",
    "            np.save(f, err_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
